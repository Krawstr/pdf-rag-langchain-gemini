Emergia-RAG: Sistema de Perguntas e Respostas com IA
Um sistema de Perguntas e Respostas focado no conceito de Emergia, constru√≠do com uma arquitetura de Gera√ß√£o Aumentada por Recupera√ß√£o (RAG) para fornecer respostas precisas e baseadas em fontes especializadas.

üìú √çndice
1. Motiva√ß√£o

2. O que √© Emergia?

3. Arquitetura do Sistema (RAG)

4. Como Funciona

Etapa 1: Indexa√ß√£o de Dados (Offline)

Etapa 2: Infer√™ncia e Gera√ß√£o (Online)

5. Stack de Tecnologias

6. Configura√ß√£o do Ambiente

7. Como Usar

8. Desafios e Solu√ß√µes

9. Pr√≥ximos Passos

10. Agradecimentos

üí° 1. Motiva√ß√£o
Tudo come√ßou com um "erro de digita√ß√£o" no meu curr√≠culo, apontado pelo meu mentor durante uma conversa. A palavra era "emergia", e n√£o "energia". Expliquei que o termo estava correto e se referia a um conceito fascinante da ecologia, frequentemente desconhecido.

Essa confus√£o me inspirou a transformar a curiosidade em um projeto pr√°tico. Unindo a sugest√£o do meu mentor de explorar agentes de IA com meu interesse no tema, decidi construir um sistema focado exclusivamente em responder perguntas sobre Emergia, garantindo que as respostas fossem precisas e confi√°veis.

üåø 2. O que √© Emergia?
Emergia (do ingl√™s, emergy ou embodied energy) √© um conceito da ecologia e da termodin√¢mica que se refere √† quantidade total de energia de um determinado tipo (geralmente solar) que foi utilizada, direta ou indiretamente, para gerar um produto, servi√ßo ou recurso. √â uma forma de avaliar o verdadeiro "custo" ambiental e energ√©tico de algo, considerando toda a cadeia de transforma√ß√µes na natureza e na economia.

üèóÔ∏è 3. Arquitetura do Sistema (RAG)
Este projeto implementa o padr√£o de arquitetura Retrieval-Augmented Generation (RAG). O objetivo do RAG √© aprimorar a qualidade das respostas de Modelos de Linguagem Grandes (LLMs), tornando-as mais confi√°veis e menos propensas a "alucina√ß√µes" (informa√ß√µes factualmente incorretas).

O fluxo funciona da seguinte forma:

Pergunta do Usu√°rio -> [1. Recupera√ß√£o de Documentos] -> [2. Inje√ß√£o de Contexto no Prompt] -> [3. Gera√ß√£o de Resposta pelo LLM] -> Resposta Final

Em vez de enviar a pergunta diretamente ao LLM, o sistema primeiro busca em uma base de conhecimento privada (neste caso, artigos e PDFs sobre Emergia) os trechos mais relevantes. Esses trechos s√£o ent√£o fornecidos ao LLM como um contexto, para que ele formule uma resposta baseada nos fatos apresentados.

üõ†Ô∏è 4. Como Funciona
O processo √© dividido em duas fases principais:

Etapa 1: Indexa√ß√£o de Dados (Offline)
Esta etapa prepara a base de conhecimento que alimentar√° o sistema. √â executada uma √∫nica vez ou sempre que a base de dados for atualizada. O script respons√°vel por esta fase √© o create_db.py.

Carregamento de Dados (Data Loading): Documentos em formato PDF, localizados em um diret√≥rio, s√£o carregados utilizando a classe PyPDFDirectoryLoader do LangChain.

Segmenta√ß√£o de Documentos (Chunking): Os documentos s√£o divididos em segmentos menores (chunks) usando RecursiveCharacterTextSplitter. Esta t√©cnica, a mais recomendada, tenta manter a coes√£o sem√¢ntica do texto ao dividi-lo em separadores l√≥gicos.

Gera√ß√£o de Embeddings (Embedding Generation): Cada chunk de texto √© convertido em um vetor num√©rico de alta dimens√£o usando o modelo models/embedding-001 da API do Google.

Armazenamento e Indexa√ß√£o (Vector Storage & Indexing): Os vetores de embedding, juntamente com os textos originais, s√£o armazenados em um banco de dados vetorial local, o ChromaDB.

Etapa 2: Infer√™ncia e Gera√ß√£o (Online)
Esta etapa ocorre em tempo real a cada pergunta do usu√°rio e √© gerenciada pelo script main.py.

Consulta do Usu√°rio: O sistema recebe uma pergunta sobre Emergia.

Vetoriza√ß√£o da Consulta: A pergunta √© convertida em um vetor de embedding usando o mesmo modelo da Etapa 1.

Busca por Similaridade: O sistema realiza uma busca no ChromaDB para encontrar os k chunks de texto mais similares √† pergunta.

Aumento do Prompt: Os chunks recuperados s√£o inseridos em um template de prompt juntamente com a pergunta original, instruindo o LLM a responder com base exclusivamente no contexto fornecido.

Gera√ß√£o da Resposta: O prompt aumentado √© enviado ao LLM (API do Gemini), que sintetiza uma resposta coesa e factual.

üöÄ 5. Stack de Tecnologias
Linguagem: Python

Orquestra√ß√£o de LLMs: LangChain

Modelo de Linguagem (LLM): Google Gemini API

Modelo de Embedding: Google models/embedding-001

Banco de Dados Vetorial: ChromaDB

Manipula√ß√£o de Dados: PyPDFDirectoryLoader, RecursiveCharacterTextSplitter

‚öôÔ∏è 6. Configura√ß√£o do Ambiente
Clone o reposit√≥rio:

git clone https://github.com/seu-usuario/emergia-rag.git
cd emergia-rag

Crie e ative um ambiente virtual:

# Linux/macOS
python3 -m venv venv
source venv/bin/activate

# Windows
python -m venv venv
.\venv\Scripts\activate

Instale as depend√™ncias:
(Crie um arquivo requirements.txt com as bibliotecas necess√°rias, como langchain, google-generativeai, chromadb, pypdf, etc.)

pip install -r requirements.txt

Configure suas credenciais:
Crie um arquivo .env na raiz do projeto e adicione sua chave da API do Google:

GOOGLE_API_KEY="SUA_CHAVE_API_AQUI"

‚ñ∂Ô∏è 7. Como Usar
Prepare a Base de Conhecimento:

Crie uma pasta chamada data na raiz do projeto.

Adicione todos os seus arquivos PDF sobre Emergia dentro desta pasta.

Execute o script de indexa√ß√£o:
Este script ir√° processar os PDFs, gerar os embeddings e criar o banco de dados vetorial.

python create_db.py

Execute a interface de consulta:
Ap√≥s a cria√ß√£o do banco, execute o script principal para come√ßar a fazer perguntas.

python main.py

O terminal ir√° exibir um prompt para voc√™ digitar sua pergunta.

üß† 8. Desafios e Solu√ß√µes
O principal desafio t√©cnico foi lidar com o limite de requisi√ß√µes da API do Google (erro 429 Too Many Requests) ao tentar vetorizar centenas de chunks de uma s√≥ vez.

Solu√ß√£o: Implementei um sistema de processamento em lotes (batch processing). Em vez de enviar todos os chunks de uma vez, o c√≥digo os agrupa em lotes de tamanho 100. O primeiro lote √© usado para criar o banco de dados, e os lotes subsequentes s√£o adicionados em um loop. Para garantir a robustez, adicionei uma pausa de 1 segundo (time.sleep(1)) entre o processamento de cada lote, evitando exceder a cota de requisi√ß√µes por minuto da API.

üéØ 9. Pr√≥ximos Passos
Este projeto foi uma excelente introdu√ß√£o √† arquitetura RAG. As melhorias planejadas incluem:

[ ] Aprimorar a Base de Dados: Realizar uma curadoria mais rigorosa dos artigos e buscar mais fontes.

[ ] Desenvolver uma Interface Gr√°fica: Criar uma GUI com Tkinter ou uma interface web com Flask/Django para tornar o sistema mais amig√°vel.

[ ] Refinar a Recupera√ß√£o: Implementar t√©cnicas mais avan√ßadas de recupera√ß√£o para melhorar a qualidade do contexto.

[ ] Avalia√ß√£o do Sistema: Criar um conjunto de dados de perguntas e respostas para avaliar objetivamente a performance do RAG.

üôè 10. Agradecimentos
Um agradecimento especial ao meu mentor, cuja provoca√ß√£o inicial foi o catalisador para este projeto. O aprendizado foi imenso.

Feedbacks e contribui√ß√µes s√£o sempre bem-vindos!
